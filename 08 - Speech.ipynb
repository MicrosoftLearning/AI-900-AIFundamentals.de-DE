{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech\r\n",
        "\r\n",
        "Wir erwarten immer öfter, mit KI-Systemen (Künstliche Intelligenz) kommunizieren zu können, indem wir mit ihnen sprechen, und erwarten oft auch eine gesprochene Antwort.\r\n",
        "\r\n",
        "![Ein sprechender Roboter](./images/speech.jpg)\r\n",
        "\r\n",
        "*Spracherkennung* (ein KI-System, das gesprochene Sprache interpretiert) und *Sprachsynthese* (ein KI-System, das eine gesprochene Antwort generiert) sind die wichtigsten Komponenten einer KI-Lösung mit Sprachunterstützung.\r\n",
        "\r\n",
        "## Erstellen einer Cognitive Services-Ressource\r\n",
        "\r\n",
        "Um eine Software zu erstellen, die gesprochene Sprache interpretiert und verbal antwortet, können Sie den Cognitive Service **Speech** verwenden, der einfache Methoden zum Transkribieren von gesprochener Sprache zu Text und umgekehrt bereitstellt.\r\n",
        "\r\n",
        "Falls noch nicht geschehen, führen Sie die folgenden Schritte aus, um eine **Cognitive Services**-Ressource in Ihrem Azure-Abonnement zu erstellen:\r\n",
        "\r\n",
        "> **Hinweis**: Falls Sie bereits eine Cognitive Services-Ressource haben, können Sie die entsprechende **Schnellstart**-Seite im Azure-Portal öffnen und den Schlüssel und den Endpunkt der Ressource unten in die Zelle kopieren. Führen Sie andernfalls die folgenden Schritte aus, um eine Ressource zu erstellen.\r\n",
        "\r\n",
        "1. Öffnen Sie das Azure-Portal unter „https://portal.azure.com“ in einer neuen Browserregisterkarte, und melden Sie sich mit Ihrem Microsoft-Konto an.\r\n",
        "2. Klicken Sie auf die Schaltfläche **&#65291;Ressource erstellen**, suchen Sie nach *Cognitive Services*, und erstellen Sie eine **Cognitive Services**-Ressource mit den folgenden Einstellungen:\r\n",
        "    - **Abonnement**: *Ihr Azure-Abonnement*\r\n",
        "    - **Ressourcengruppe**: *Wählen Sie eine Ressourcengruppe aus, oder erstellen Sie eine Ressourcengruppe mit einem eindeutigen Namen.*\r\n",
        "    - **Region**: *Wählen Sie eine verfügbare Region aus*:\r\n",
        "    - **Name**: *Geben Sie einen eindeutigen Namen ein.*\r\n",
        "    - **Tarif**: S0\r\n",
        "    - **Ich bestätige, dass ich die Hinweise gelesen und verstanden habe**: Ausgewählt\r\n",
        "3. Warten Sie, bis die Bereitstellung abgeschlossen ist. Öffnen Sie anschließend Ihre Cognitive Services-Ressource, und klicken Sie auf der Seite **Übersicht** auf den Link zur Schlüsselverwaltung für den Dienst. Sie benötigen den Schlüssel und den Speicherort, um sich aus Clientanwendungen heraus mit Ihrer Cognitive Services-Ressource zu verbinden.\r\n",
        "\r\n",
        "### Abrufen des Schlüssels und des Speicherorts für Ihre Cognitive Services-Ressource\r\n",
        "\r\n",
        "Um Ihre Cognitive Services-Ressource verwenden zu können, benötigen Clientanwendungen deren Authentifizierungsschlüssel und Speicherort:\r\n",
        "\r\n",
        "1. Kopieren Sie im Azure-Portal auf der Seite **Schlüssel und Endpunkt** für Ihre Cognitive Service-Ressource den **Schlüssel1** für Ihre Ressource, und fügen Sie ihn im unten stehenden Code anstelle von **YOUR_COG_KEY** ein.\r\n",
        "2. Kopieren Sie den **Speicherort** für Ihre Ressource, und fügen Sie ihn unten im Code anstelle von **YOUR_COG_LOCATION** ein.\r\n",
        ">**Hinweis**: Bleiben Sie auf der Seite **Schlüssel und Endpunkt**, und kopieren Sie den **Speicherort** von dieser Seite (Beispiel: _westus_). Fügen Sie KEINE Leerzeichen zwischen den Wörtern im Feld „Speicherort“ ein. \r\n",
        "3. Führen Sie den folgenden Code aus, indem Sie links neben der Zelle auf die Schaltfläche **Zelle ausführen** (&#9655;) klicken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695240794
        }
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spracherkennung\r\n",
        "\r\n",
        "Angenommen, Sie möchten ein Gebäudeautomatisierungssystem entwickeln, das Sprachbefehle wie etwa „Licht einschalten“ oder „Licht ausschalten“ erkennt. Ihre Anwendung muss in der Lage sein, audiobasierte Eingaben (Ihre Sprachbefehle) zu interpretieren, indem sie sie zu Text transkribiert, der anschließend gelesen und analysiert werden kann.\r\n",
        "\r\n",
        "Jetzt können Sie damit anfangen, Spracheingaben zu transkribieren. Die Eingabe kann entweder von einem **Mikrofon** oder aus einer **Audiodatei** stammen. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spracherkennung mit einer Audiodatei\r\n",
        "\r\n",
        "Führen Sie die folgende Zelle aus, um den Spracherkennungsdienst mit einer **Audiodatei** zu testen. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from playsound import playsound\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "\n",
        "# Get spoken command from audio file\r\n",
        "file_name = 'light-on.wav'\n",
        "audio_file = os.path.join('data', 'speech', file_name)\n",
        "\n",
        "# Configure speech recognizer\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "# Use a one-time, synchronous call to transcribe the speech\r\n",
        "speech = speech_recognizer.recognize_once()\n",
        "\n",
        "# Play the original audio file\r\n",
        "playsound(audio_file)\n",
        "\n",
        "# Show transcribed text from audio file\r\n",
        "print(speech.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sprachsynthese\r\n",
        "\r\n",
        "Sie haben also gesehen, wie der Speech-Dienst gesprochene Wörter zu Text transkribieren kann, aber funktioniert dies auch umgekehrt? Wie können Sie Text zu Sprache konvertieren?\r\n",
        "\r\n",
        "Angenommen, Ihr Gebäudeautomatisierungssystem hat einen Befehl erhalten, das Licht einzuschalten. Eine passende Antwort wäre beispielsweise eine verbale Bestätigung des Befehls (und natürlich die Ausführung des Befehls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599695261170
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
        "%matplotlib inline\n",
        "\n",
        "# Get text to be spoken\r\n",
        "response_text = 'Turning the light on.'\n",
        "\n",
        "# Configure speech synthesis\r\n",
        "speech_config = SpeechConfig(cog_key, cog_location)\n",
        "speech_config.speech_synthesis_voice_name = 'en-US-ChristopherNeural'\n",
        "speech_synthesizer = SpeechSynthesizer(speech_config)\n",
        "\n",
        "# Transcribe text into speech\r\n",
        "result = speech_synthesizer.speak_text(response_text)\n",
        "\n",
        "# Display an appropriate image \r\n",
        "file_name = response_text.lower() + \"jpg\"\n",
        "img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n",
        "plt.axis('off')\n",
        "plt. imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ändern Sie die Variable **response_text** zu *Licht wird ausgeschaltet.* (inklusive Punkt am Ende), und führen Sie die Zelle erneut aus, um das Ergebnis zu hören.\r\n",
        "\r\n",
        "## Weitere Informationen\r\n",
        "\r\n",
        "In diesem Notebook haben Sie ein sehr einfaches Beispiel für die Nutzung des Cognitive Service „Speech“ gesehen. Weitere Informationen zu den Funktionen [Sprache-in-Text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) und [Text-zu-Sprache](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) finden Sie in der Dokumentation für den Speech-Dienst."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 32-bit",
      "metadata": {
        "interpreter": {
          "hash": "177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"
        }
      },
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}