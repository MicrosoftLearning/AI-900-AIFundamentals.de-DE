{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objekterkennung\r\n",
        "\r\n",
        "Die *Objekterkennung* ist eine Art von maschinellem Sehen, bei dem ein Machine Learning-Modell trainiert wird, um Instanzen von Objekten in einem Bild zu erkennen und deren Position mit einem *Begrenzungsrahmen* zu markieren. Sie können sich die Objekterkennung als eine Weiterentwicklung der *Bildklassifizierung* vorstellen (Was ist auf diesem Bild zu sehen?). Hiermit können wir Lösungen erstellen, die ermitteln, welche Objekte auf einem Bild zu sehen sind und an welcher Position sie sich befinden.\r\n",
        "\r\n",
        "![Ein Roboter, der Obst identifiziert](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Ein Lebensmittelgeschäft kann beispielsweise ein Objekterkennungsmodell einsetzen, um ein automatisiertes Kassensystem zu entwickeln, das ein Förderband mit einer Kamera scannt und Waren erkennt, ohne dass diese einzeln auf das Band gelegt und gescannt werden müssen.\r\n",
        "\r\n",
        "Der Cognitive Service **Custom Vision** in Microsoft Azure bietet eine cloudbasierte Lösung zum Erstellen und Veröffentlichen von benutzerdefinierten Objekterkennungsmodellen.\r\n",
        "\r\n",
        "## Erstellen einer Custom Vision-Ressource\r\n",
        "\r\n",
        "Um den Custom Vision-Dienst nutzen zu können, benötigen Sie eine Azure-Ressource, mit der Sie ein Modell trainieren können, und eine Ressource, mit der Sie den Dienst veröffentlichen und für Anwendungen bereitstellen können. Sie können entweder dieselbe Ressource für diese Aufgaben verwenden, oder Sie können unterschiedliche Ressourcen verwenden, um die Kosten separat zu verwalten, sofern die Ressourcen in derselben Region erstellt wurden. Für diese Aufgaben können Sie entweder eine allgemeine **Cognitive Services**-Ressource oder eine spezifische **Custom Vision**-Ressource verwenden. Gehen Sie wie folgt vor, um eine neue **Custom Vision**-Ressource zu erstellen. Sie können bei Bedarf auch eine vorhandene Ressource verwenden.\r\n",
        "\r\n",
        "1. Öffnen Sie das Azure-Portal unter [https://portal.azure.com](https://portal.azure.com) in einer neuen Browserregisterkarte, und melden Sie sich mit dem Microsoft-Konto an, das Ihrem Azure-Abonnement zugeordnet ist.\r\n",
        "2. Wählen Sie die Schaltfläche **&#65291;Ressource erstellen** aus, suchen Sie nach *custom vision*, und erstellen Sie eine **Custom Vision**-Ressource mit den folgenden Einstellungen:\r\n",
        "    - **Erstellungsoptionen**: Beide\r\n",
        "    - **Abonnement**: *Ihr Azure-Abonnement*\r\n",
        "    - **Ressourcengruppe**: *Wählen Sie eine Ressourcengruppe aus, oder erstellen Sie eine Ressourcengruppe mit einem eindeutigen Namen.*\r\n",
        "    - **Name**: *Geben Sie einen eindeutigen Namen ein.*\r\n",
        "    - **Speicherort für das Training**: *Wählen Sie eine verfügbare Region aus.*\r\n",
        "    - **Tarif für Training**: F0\r\n",
        "    - **Speicherort für die Vorhersage**: *Entspricht der Trainingsressource*\r\n",
        "    - **Tarif für Vorhersage**: F0\r\n",
        "\r\n",
        "    > **Hinweis**: Falls Ihr Abonnement bereits einen F0 Custom Vision-Dienst enthält, wählen Sie hier **S0** aus.\r\n",
        "\r\n",
        "3. Warten Sie, bis die Ressource erstellt wurde.\r\n",
        "\r\n",
        "## Erstellen eines Custom Vision-Projekts\r\n",
        "\r\n",
        "Um ein Objekterkennungsmodell zu trainieren, müssen Sie ein Custom Vision-Projekt mit Ihrer Trainingsressource erstellen. Dazu verwenden Sie das Custom Vision-Portal.\r\n",
        "\r\n",
        "1. Öffnen Sie das Custom Vision-Portal unter [https://customvision.ai](https://customvision.ai) in einer neuen Browserregisterkarte, und melden Sie sich mit dem Microsoft-Konto an, das Ihrem Azure-Abonnement zugeordnet ist.\r\n",
        "2. Erstellen Sie ein neues Projekt mit den folgenden Einstellungen:\r\n",
        "    - **Name**: Erkennung von Lebensmitteln\r\n",
        "    - **Beschreibung**: Objekterkennung für Lebensmittel\r\n",
        "    - **Ressource**: *Die zuvor erstellte Custom Vision-Ressource*\r\n",
        "    - **Projekttypen**: Objekterkennung\r\n",
        "    - **Domänen**: Allgemein\r\n",
        "3. Warten Sie, bis das Projekt erstellt und im Browser geöffnet wurde.\r\n",
        "\r\n",
        "## Hinzufügen und Kennzeichnen von Bildern\r\n",
        "\r\n",
        "Um ein Objekterkennungsmodell zu trainieren, müssen Sie Bilder hochladen, die die Klassen enthalten, die das Modell identifizieren soll. Anschließend müssen Sie die Bilder mit einem Tag kennzeichnen, um Begrenzungsrahmen für die einzelnen Objektinstanzen anzugeben.\r\n",
        "\r\n",
        "1. Laden Sie die Trainingsbilder unter „https://aka.ms/fruit-objects“ herunter, und extrahieren Sie sie. Der extrahierte Ordner enthält eine Sammlung von Bildern mit Obst. **Hinweis:** Falls Sie nicht auf die Training-Images zugreifen können, navigieren Sie als temporäre Problemumgehung zu https://www.github.com, then go to https://aka.ms/fruit-objects. \r\n",
        "2. Vergewissern Sie sich im Custom Vision-Portal [https://customvision.ai](https://customvision.ai), dass Sie sich in Ihrem Objekterkennungsprojekt _Erkennung von Lebensmitteln_ befinden. Wählen Sie anschließend die Option **Bilder hinzufügen** aus, und laden Sie alle Bilder aus dem extrahierten Ordner hoch.\r\n",
        "\r\n",
        "![Laden Sie die heruntergeladenen Bilder hoch, indem Sie auf „Bilder hinzufügen“ klicken.](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. Warten Sie, bis die Bilder hochgeladen wurden, und wählen Sie das erste Bild aus, um es zu öffnen.\r\n",
        "4. Fahren Sie mit der Maus über alle Objekte im Bild, bis eine automatisch erkannte Region angezeigt wird (siehe Bild unten). Wählen Sie anschließend das Objekt aus, und passen Sie die Größe der Region um das Objekt herum bei Bedarf an.\r\n",
        "\r\n",
        "![Die Standardregion für ein Objekt](./images/object-region.jpg)\r\n",
        "\r\n",
        "Alternativ können Sie einen Bereich um das Objekt herum ziehen, um eine Region zu erstellen.\r\n",
        "\r\n",
        "5. Wenn die Region das Objekt umgibt, fügen sie ein neues Tag mit dem entsprechenden Objekttyp hinzu (*Apfel*, *Banane* oder *Orange*), so wie hier gezeigt:\r\n",
        "\r\n",
        "![Ein mit Tag markiertes Objekt in einem Bild](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Wählen Sie die restlichen Objekte im Bild aus, markieren Sie diese mit einem Tag, passen Sie die Größe der jeweiligen Region an, und fügen Sie neue Tags wie erforderlich hinzu.\r\n",
        "\r\n",
        "![Zwei mit Tags markierte Objekte in einem Bild](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Mit dem Link **>** auf der rechten Seite können Sie zum nächsten Bild wechseln und die dort enthaltenen Objekte markieren. Bearbeiten Sie die gesamte Bildersammlung auf diese Weise, indem Sie alle Äpfel, Bananen und Orangen markieren.\r\n",
        "\r\n",
        "8. Wenn Sie alle Bilder markiert haben, schließen Sie den **Bilddetail**-Editor, und wählen Sie auf der Seite **Trainingsbilder** unter **Tags** die Option **Markiert** aus, um Ihre markierten Bilder anzuzeigen:\r\n",
        "\r\n",
        "![Mit Tags markierte Bilder in einem Projekt](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Trainieren und Testen eines Modells\r\n",
        "\r\n",
        "Nachdem Sie die Bilder in Ihrem Projekt markiert haben, können Sie ein Modell trainieren.\r\n",
        "\r\n",
        "1. Klicken Sie im Custom Vision-Projekt auf **Trainieren**, um ein Objekterkennungsmodell mit den markierten Bildern zu trainieren. Wählen Sie die Option **Schnelltraining** aus.\r\n",
        "2. Warten Sie, bis der Vorgang abgeschlossen wurde (kann etwa zehn Minuten dauern), und überprüfen Sie die Leistungsmetriken *Genauigkeit*, *Abruf* und *mAP*. Diese Metriken messen die Vorhersagegenauigkeit des Klassifizierungsmodells und sollten jeweils den Wert „Hoch“ haben.\r\n",
        "3. Klicken Sie oben rechts auf der Seite auf **Schnelltest**, geben Sie `https://aka.ms/apple-orange` in das Feld **Bild-URL** ein, und sehen Sie sich die generierte Vorhersage an. Schließen Sie dann das Fenster **Schelltest**.\r\n",
        "\r\n",
        "## Veröffentlichen und Verwenden des Objekterkennungsmodells\r\n",
        "\r\n",
        "Sie können Ihr trainiertes Modell jetzt veröffentlichen und aus einer Clientanwendung heraus verwenden.\r\n",
        "\r\n",
        "1. Klicken Sie oben links auf der Seite **Leistung** auf **&#128504; Veröffentlichen**, um das trainierte Modell mit den folgenden Einstellungen zu veröffentlichen:\r\n",
        "    - **Modellname**: Lebensmittelerkennung\r\n",
        "    - **Vorhersageressource**: *Ihre Custom Vision-**Vorhersageressource***.\r\n",
        "\r\n",
        "### (!) Überprüfung \r\n",
        "Haben Sie den gleichen Modellnamen (**Lebensmittelerkennung**) verwendet? \r\n",
        "\r\n",
        "2. Klicken Sie nach dem Veröffentlichen auf das Symbol *Einstellungen* (&#9881;) oben rechts auf der Seite **Leistung**, um die Projekteinstellungen zu öffnen. Kopieren Sie anschließend unter **Allgemein** (linke Seite) die **Projekt-ID**. Scrollen Sie nach unten, und fügen Sie sie in die Codezelle unter Schritt 5 anstelle von **YOUR_PROJECT_ID** ein. \r\n",
        "\r\n",
        "> (*Falls Sie am Anfang dieses Labs eine **Cognitive Services**-Ressource verwendet haben, anstatt eine **Custom Vision**-Ressource zu erstellen, können Sie den Schlüssel und den Endpunkt auf der rechten Seite der Projekteinstellungen kopieren, in die unten stehende Codezelle einfügen und sie anschließend ausführen, um das Ergebnis anzuzeigen. Führen Sie andernfalls die folgenden Schritte aus, um den Schlüssel und den Endpunkt für Ihre Custom Vision-Vorhersageressource abzurufen*).\r\n",
        "\r\n",
        "3. Klicken Sie links oben auf der Seite **Projekteinstellungen** auf das Symbol für den *Projektkatalog* (&#128065;), um zur Startseite des Custom Vision-Portals zu gelangen, auf der Ihr Projekt jetzt aufgelistet wird.\r\n",
        "\r\n",
        "4. Klicken Sie auf der Startseite des Custom Vision-Portals oben rechts auf das Symbol *Einstellungen* (&#9881;), um die Einstellungen für Ihren Custom Vision-Dienst anzuzeigen. Erweitern Sie anschließend unter **Ressourcen** Ihre *Vorhersageressource* (<u>nicht</u> die Trainingsressource), und kopieren Sie die Werte unter **Schlüssel** und **Endpunkt** in die Codezelle unter Schritt 5. Ersetzen Sie dabei **YOUR_KEY** und **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Überprüfung \r\n",
        "Falls Sie eine **Custom Vision**-Ressource verwenden: Haben Sie die **Vorhersageressource** verwendet (<u>nicht</u> die Trainingsressource)?\r\n",
        "\r\n",
        "5. Führen Sie die folgende Codezelle aus, indem Sie oben links in der Zelle auf die Schaltfläche „Zelle ausführen“ <span>&#9655;</span> klicken, um Ihre Werte für Projekt-ID, Schlüssel und Endpunkt als Variablenwerte festzulegen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anschließend können Sie Ihren Schlüssel und Endpunkt in einem Custom Vision-Client verwenden, um sich mit Ihrem Custom Vision-Objekterkennungsmodell zu verbinden.\r\n",
        "\r\n",
        "Führen Sie die folgende Codezelle aus, die Ihr Modell verwendet, um einzelne Lebensmittel in einem Bild zu erkennen.\r\n",
        "\r\n",
        "> **Hinweis**: Sorgen Sie sich nicht zu sehr um die Codedetails. Der Code verwendet das Python-SDK für den Custom Vision-Dienst, um ein Bild an Ihr Modell zu übermitteln und Vorhersagen für die erkannten Objekte zu erhalten. Jede Vorhersage besteht aus einem Klassennamen (*Apfel*, *Banane* oder *Orange*) und Koordinaten für einen *Begrenzungsrahmen*, um anzugeben, wo sich das erkannte Objekt innerhalb des Bilds befindet. Der Code verwendet diese Informationen, um im Bild beschriftete Rahmen um die einzelnen Objekte zu zeichnen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In den resultierenden Vorhersagen sehen Sie die erkannten Objekte und die Wahrscheinlichkeiten für die einzelnen Vorhersagen."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}