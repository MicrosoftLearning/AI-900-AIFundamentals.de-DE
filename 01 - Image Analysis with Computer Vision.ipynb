{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bildanalyse mit dem Dienst für maschinelles Sehen\r\n",
        "\r\n",
        "![Ein Roboter, der ein Bild hält](./images/computer_vision.jpg)\r\n",
        "\r\n",
        "*Maschinelles Sehen* ist ein Teilbereich der künstlichen Intelligenz (KI) für die Entwicklung von KI-Systemen, die die Welt „sehen“ können, entweder in Echtzeit durch eine Kamera oder durch die Analyse von Bildern und Video. Dies ist dadurch möglich, dass Bilder im Grunde genommen nur Arrays von numerischen Pixelwerten sind. Mit diesen Pixelwerten als *Merkmale* können wir Machine Learning-Modelle trainieren, die Bilder klassifizieren, einzelne Objekte in einem Bild erkennen und sogar textbasierte Zusammenfassungen von Fotos generieren.\r\n",
        "\r\n",
        "## Verwenden des Cognitive Service für maschinelles Sehen\r\n",
        "\r\n",
        "Microsoft Azure enthält verschiedene *Cognitive Services*, die gängige KI-Funktionen kapseln, mit denen Sie unter anderem auch Lösungen für maschinelles Sehen entwickeln können.\r\n",
        "\r\n",
        "Der Cognitive Service für *maschinelles Sehen* dient als offensichtlicher Ausgangspunkt für unsere Erkundung von maschinellem Sehen in Azure. Er verwendet vorab trainierte Machine Learning-Modelle, um Bilder zu analysieren und Informationen zu extrahieren.\r\n",
        "\r\n",
        "Angenommen, Northwind Traders möchte einen „Smart Store“ implementieren, der von KI überwacht wird, um zu identifizieren, welche Kunden Unterstützung benötigen, und Mitarbeiter zu diesen Kunden zu schicken. Der Dienst für maschinelles Sehen kann Bilder von den Kameras im Geschäft analysieren und aussagekräftige Beschreibungen der Bilder generieren.\r\n",
        "\r\n",
        "### Erstellen einer Cognitive Services-Ressource\r\n",
        "\r\n",
        "Erstellen Sie zunächst eine **Cognitive Services**-Ressource in Ihrem Azure-Abonnement:\r\n",
        "\r\n",
        "1. Öffnen Sie das Azure-Portal unter „https://portal.azure.com“ in einer neuen Browserregisterkarte, und melden Sie sich mit Ihrem Microsoft-Konto an.\r\n",
        "2. Klicken Sie auf die Schaltfläche **&#65291;Ressource erstellen**, suchen Sie nach *Cognitive Services*, und erstellen Sie eine **Cognitive Services**-Ressource mit den folgenden Einstellungen:\r\n",
        "    * **Abonnement**: *Ihr Azure-Abonnement*\r\n",
        "    * **Ressourcengruppe**: *Wählen Sie eine Ressourcengruppe aus, oder erstellen Sie eine Ressourcengruppe mit einem eindeutigen Namen.*\r\n",
        "    * **Region**: *Wählen Sie eine verfügbare Region aus*:\r\n",
        "    * **Name**: *Geben Sie einen eindeutigen Namen ein.*\r\n",
        "    * **Tarif**: S0\r\n",
        "    * **Ich bestätige, dass ich die Hinweise gelesen und verstanden habe**: Ausgewählt\r\n",
        "3. Warten Sie, bis die Bereitstellung abgeschlossen ist. Öffnen Sie anschließend Ihre Cognitive Services-Ressource, und klicken Sie auf der Seite **Übersicht** auf den Link zur Schlüsselverwaltung für den Dienst. Sie benötigen den Endpunkt und Schlüssel, um sich aus Clientanwendungen heraus mit Ihrer Cognitive Services-Ressource zu verbinden.\r\n",
        "\r\n",
        "### Abrufen des Schlüssels und Endpunkts für Ihre Cognitive Services-Ressource\r\n",
        "\r\n",
        "Um Ihre Cognitive Services-Ressource verwenden zu können, benötigen Clientanwendungen deren Endpunkt und Authentifizierungsschlüssel:\r\n",
        "\r\n",
        "1. Kopieren Sie im Azure-Portal auf der Seite **Schlüssel und Endpunkt** für Ihre Cognitive Service-Ressource den **Schlüssel1** für Ihre Ressource, und fügen Sie ihn im unten stehenden Code anstelle von **YOUR_COG_KEY** ein.\r\n",
        "2. Kopieren Sie den **Endpunkt** für Ihre Ressource, und fügen Sie ihn unten im Code anstelle von **YOUR_COG_ENDPOINT** ein.\r\n",
        "3. Führen Sie den unten stehenden Code aus, indem Sie die Zelle auswählen und auf die Schaltfläche **Zelle ausführen** (&#9655;) links neben der Zelle klicken."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691487445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nachdem Sie Schlüssel und Endpunkt eingerichtet haben, können Sie den Dienst für maschinelles Sehen verwenden, um Bilder zu analysieren.\r\n",
        "\r\n",
        "Führen Sie die folgende Zelle aus, um eine Beschreibung des Bilds in der Datei */data/vision/store_cam1.jpg* zu erhalten."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import vision\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Get a description from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_caption(image_path, description)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691518279
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diese Beschreibung erscheint halbwegs brauchbar.\r\n",
        "\r\n",
        "Probieren Sie ein anderes Bild aus."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam2.jpg')\n",
        "\n",
        "# Get a description from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_caption(image_path, description)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691524330
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die vorgeschlagene Beschreibung scheint ebenfalls recht genau zu sein.\r\n",
        "\r\n",
        "## Analysieren von Bildmerkmalen\r\n",
        "\r\n",
        "Bisher haben Sie mit dem Dienst für maschinelles Sehen eine Beschreibung für einige Bilder generiert, aber Sie können noch viel mehr erreichen. Mit den Analysefunktionen des Diensts für maschinelles Sehen können Sie ausführliche Informationen extrahieren, wie etwa:\r\n",
        "\r\n",
        "* Die Positionen bestimmter Objekte, die im Bild erkannt wurden.\r\n",
        "* Ort und geschätztes Alter von menschlichen Gesichtern im Bild.\r\n",
        "* Angabe, ob ein Bild nicht jugendfreie, freizügige oder gewalttätige Inhalte enthält.\r\n",
        "* Relevante Tags, die in einer Datenbank dem Bild zugeordnet werden können, um es leichter auffindbar zu machen.\r\n",
        "\r\n",
        "Führen Sie den folgenden Code aus, um ein Bild eines Käufers zu analysieren."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path to an image file\r\n",
        "image_path = os.path.join('data', 'vision', 'store_cam1.jpg')\n",
        "\n",
        "# Specify the features we want to analyze\r\n",
        "features = ['Description', 'Tags', 'Adult', 'Objects', 'Faces']\n",
        "\n",
        "# Get an analysis from the computer vision service\r\n",
        "image_stream = open(image_path, \"rb\")\n",
        "analysis = computervision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
        "\n",
        "# Show the results of analysis (code in helper_scripts/vision.py)\r\n",
        "vision.show_image_analysis(image_path, analysis)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599691530747
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weitere Informationen\r\n",
        "\r\n",
        "Neben den in diesem Notebook behandelten Funktionen haben Sie mit dem Cognitive Service für maschinelles Sehen auch die folgenden Möglichkeiten:\r\n",
        "\r\n",
        "* Berühmtheiten in Bildern erkennen.\r\n",
        "* Markenlogos in einem Bild erkennen.\r\n",
        "* Text in einem Bild mit optischer Zeichenerkennung (Optical Character Recognition, OCR) lesen.\r\n",
        "\r\n",
        "Weitere Informationen zum Cognitive Service für maschinelles Sehen finden Sie in der [Dokumentation zum maschinellen Sehen](https://docs.microsoft.com/azure/cognitive-services/computer-vision/).\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}