{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optische Zeichenerkennung\r\n",
        "\r\n",
        "![Ein Roboter liest eine Zeitung](./images/ocr.jpg)\r\n",
        "\r\n",
        "Maschinelles Sehen wird oft eingesetzt, um Text in einem Bild zu erkennen und zu interpretieren. Diese Art der Verarbeitung wird oft auch als *optische Zeichenerkennung (Optical Character Recognition, OCR)* bezeichnet.\r\n",
        "\r\n",
        "## Verwenden des Diensts für maschinelles Sehen, um Text in einem Bild zu lesen\r\n",
        "\r\n",
        "Der Cognitive Service für **maschinelles Sehen** unterstützt verschiedene OCR-Aufgaben, darunter:\r\n",
        "\r\n",
        "- Eine **OCR**-API, mit der Sie Text in mehreren Sprachen lesen können. Diese API kann synchron verwendet werden und eignet sich hervorragend, um kleine Mengen von Text in einem Bild zu erkennen und zu lesen.\r\n",
        "- Eine **Lese**-API, die für größere Dokumente optimiert ist. Diese API wird asynchron verwendet und kann sowohl für gedruckten als auch für handgeschriebenen Text verwendet werden.\r\n",
        "\r\n",
        "Sie können diesen Dienst verwenden, indem Sie entweder eine Ressource für **maschinelles Sehen** oder eine **Cognitive Services**-Ressource erstellen.\r\n",
        "\r\n",
        "Falls noch nicht geschehen, erstellen Sie eine **Cognitive Services**-Ressource in Ihrem Azure-Abonnement.\r\n",
        "\r\n",
        "> **Hinweis**: Falls Sie bereits eine Cognitive Services-Ressource haben, können Sie die entsprechende **Schnellstart**-Seite im Azure-Portal öffnen und den Schlüssel und den Endpunkt der Ressource unten in die Zelle kopieren. Führen Sie andernfalls die folgenden Schritte aus, um eine Ressource zu erstellen.\r\n",
        "\r\n",
        "1. Öffnen Sie das Azure-Portal unter „https://portal.azure.com“ in einer neuen Browserregisterkarte, und melden Sie sich mit Ihrem Microsoft-Konto an.\r\n",
        "\r\n",
        "2. Klicken Sie auf die Schaltfläche **&#65291;Ressource erstellen**, suchen Sie nach *Cognitive Services*, und erstellen Sie eine **Cognitive Services**-Ressource mit den folgenden Einstellungen:\r\n",
        "    - **Abonnement**: *Ihr Azure-Abonnement*\r\n",
        "    - **Ressourcengruppe**: *Wählen Sie eine Ressourcengruppe aus, oder erstellen Sie eine Ressourcengruppe mit einem eindeutigen Namen.*\r\n",
        "    - **Region**: *Wählen Sie eine verfügbare Region aus*:\r\n",
        "    - **Name**: *Geben Sie einen eindeutigen Namen ein.*\r\n",
        "    - **Tarif**: S0\r\n",
        "    - **Ich bestätige, dass ich die Hinweise gelesen und verstanden habe**: Ausgewählt\r\n",
        "3. Warten Sie, bis die Bereitstellung abgeschlossen ist. Öffnen Sie anschließend Ihre Cognitive Services-Ressource, und klicken Sie auf der Seite **Übersicht** auf den Link zur Schlüsselverwaltung für den Dienst. Sie benötigen den Endpunkt und Schlüssel, um sich aus Clientanwendungen heraus mit Ihrer Cognitive Services-Ressource zu verbinden.\r\n",
        "\r\n",
        "### Abrufen des Schlüssels und Endpunkts für Ihre Cognitive Services-Ressource\r\n",
        "\r\n",
        "Um Ihre Cognitive Services-Ressource verwenden zu können, benötigen Clientanwendungen deren Endpunkt und Authentifizierungsschlüssel:\r\n",
        "\r\n",
        "1. Kopieren Sie im Azure-Portal auf der Seite **Schlüssel und Endpunkt** für Ihre Cognitive Service-Ressource den **Schlüssel1** für Ihre Ressource, und fügen Sie ihn im unten stehenden Code anstelle von **YOUR_COG_KEY** ein.\r\n",
        "2. Kopieren Sie den **Endpunkt** für Ihre Ressource, und fügen Sie ihn unten im Code anstelle von **YOUR_COG_ENDPOINT** ein.\r\n",
        "3. Führen Sie die folgende Codezelle aus, indem Sie links neben der Zelle auf die Schaltfläche **Zelle ausführen** (&#9655;) klicken."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nachdem Sie nun den Schlüssel und Endpunkt eingerichtet haben, können Sie Ihren Dienst für maschinelles Sehen verwenden, um Text aus Bildern zu extrahieren.\r\n",
        "\r\n",
        "Wir beginnen zunächst mit der **OCR**-API, mit der Sie Bilder synchron analysieren und den enthaltenen Text lesen können. In diesem Fall verwenden wir ein Werbebild mit etwas Text für das fiktive Einzelhandelsunternehmen Northwind Traders. Führen Sie die folgende Zelle aus, um den Text zu lesen. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der im Bild gefundene Text wird in eine hierarchische Struktur von Regionen, Zeilen und Wörtern eingeteilt. Der Code liest diese Struktur, um die Ergebnisse abzurufen.\r\n",
        "\r\n",
        "In den Ergebnissen wird der gelesene Text über dem Bild angezeigt. \r\n",
        "\r\n",
        "## Anzeigen von Begrenzungsrahmen\r\n",
        "\r\n",
        "Die Ergebnisse enthalten außerdem Koordinaten für *Begrenzungsrahmen* um die im Bild gefundenen Textzeilen und Wörter. Führen Sie die folgende Zelle aus, um die Begrenzungsrahmen für die Textzeilen aus dem oben verwendeten Werbebild anzuzeigen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Öffnen Sie ein Bild, um es anzuzeigen.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Show the position of the line of text\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Show the image with the text locations highlighted\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im Ergebnis werden die Begrenzungsrahmen für die einzelnen Textzeilen als Rechteck im Bild angezeigt.\r\n",
        "\r\n",
        "## Verwenden der Lese-API\r\n",
        "\r\n",
        "Die zuvor verwendete OCR-API eignet sich gut für Bilder mit kleineren Mengen an Text. Für größere Textmengen wie etwa gescannte Dokumente können Sie die **Lese**-API verwenden. Hierzu sind mehrere Schritte erforderlich:\r\n",
        "\r\n",
        "1. Übermitteln Sie ein Bild an den Dienst für maschinelles Sehen, um den Text asynchron zu lesen und zu analysieren.\r\n",
        "2. Warten Sie, bis der Analysevorgang abgeschlossen ist.\r\n",
        "3. Rufen Sie die Ergebnisse der Analyse ab.\r\n",
        "\r\n",
        "Führen Sie die folgende Zelle aus, um mit diesem Vorgang den Text in einem gescannten Brief an den Filialleiter eines Northwind Traders-Geschäfts zu lesen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Überprüfen Sie die Ergebnisse. Sie erhalten eine komplette Abschrift des Briefs, der hauptsächlich aus gedrucktem Text und einer handschriftlichen Unterschrift besteht. Das ursprüngliche Bild des Briefs wird unter den OCR-Ergebnissen angezeigt. (Möglicherweise müssen Sie scrollen, um es zu sehen.)\r\n",
        "\r\n",
        "## Lesen von handschriftlichem Text\r\n",
        "\r\n",
        "Im vorherigen Beispiel haben wir in der Anforderung für die Bildanalyse einen Texterkennungsmodus angegeben, der den Vorgang für *gedruckten* Text optimiert. Beachten Sie, dass die handschriftliche Unterschrift trotzdem gelesen wurde.\r\n",
        "\r\n",
        "Diese Fähigkeit, handschriftlichen Text zu lesen, ist extrem nützlich. Stellen Sie sich beispielsweise vor, Sie haben eine Einkaufsliste auf einen Notizzettel geschrieben und möchten Sie mit einer App auf Ihr Telefon einlesen und den enthaltenen Text erkennen.\r\n",
        "\r\n",
        "Führen Sie die folgende Zelle aus, um einen Beispielvorgang für das Lesen einer handschriftlichen Einkaufsliste anzuzeigen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weitere Informationen\r\n",
        "\r\n",
        "Weitere Informationen zur Verwendung des Cognitive Service für OCR finden Sie in der [Dokumentation zum maschinellen Sehen](https://docs.microsoft.com/de-de/azure/cognitive-services/computer-vision/concept-recognizing-text)."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}